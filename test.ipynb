{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3763ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55100/3924845034.py:26: FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
      "  df = pd.read_csv(\n",
      "/tmp/ipykernel_55100/3924845034.py:26: FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
      "  df = pd.read_csv(\n",
      "/tmp/ipykernel_55100/3924845034.py:26: FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File caricato con successo: data/fake/AAPLUSUSD_M5.csv\n",
      "File caricato con successo: data/fake/NFLXUSUSD_M5.csv\n",
      "File caricato con successo: data/fake/TSLAUSUSD_M5.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from fbm import FBM\n",
    "from arch import arch_model\n",
    "from scipy.linalg import cholesky\n",
    "from scipy.optimize import minimize\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "def load_and_clean_file(file_path):\n",
    "    \"\"\"\n",
    "    Carica un file di dati finanziari in formato .txt o .csv, dove i valori\n",
    "    sono separati da tabulatori.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Il percorso del file da caricare.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un DataFrame pulito e pronto per l'analisi.\n",
    "    \"\"\"\n",
    "    # Legge il file usando '\\t' come delimitatore.\n",
    "    # Combina le prime due colonne (indice 0 e 1) in un'unica colonna\n",
    "    # chiamata 'datetime' e la imposta come indice.\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        delimiter='\\t',\n",
    "        header=0,\n",
    "        parse_dates={'datetime': [0]},\n",
    "        index_col='datetime'\n",
    "    )\n",
    "    \n",
    "    # Standardizza i nomi delle colonne in minuscolo\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "\n",
    "    print(f\"File caricato con successo: {file_path}\")\n",
    "    return df\n",
    "\n",
    "file1 = \"data/fake/AAPLUSUSD_M5.csv\"\n",
    "file2 = \"data/fake/NFLXUSUSD_M5.csv\"\n",
    "file3 = \"data/fake/TSLAUSUSD_M5.csv\"\n",
    "\n",
    "df_aapl = load_and_clean_file(file1)\n",
    "df_nflx = load_and_clean_file(file2)\n",
    "df_tsla = load_and_clean_file(file3)\n",
    "\n",
    "df_aapl.to_csv('data/fake/df_aapl.csv', sep=',', index=True, header=True)\n",
    "df_nflx.to_csv('data/fake/df_nflx.csv', sep=',', index=True, header=True)\n",
    "df_tsla.to_csv('data/fake/df_tsla.csv', sep=',', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d420602f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcolata log-RV per frequenza '1D'. Trovate 1289 osservazioni.\n",
      "Calcolata log-RV per frequenza '1h'. Trovate 8986 osservazioni.\n",
      "Calcolata log-RV per frequenza '5min'. Trovate 98891 osservazioni.\n",
      "Calcolata log-RV per frequenza '1D'. Trovate 1289 osservazioni.\n",
      "Calcolata log-RV per frequenza '1h'. Trovate 8984 osservazioni.\n",
      "Calcolata log-RV per frequenza '5min'. Trovate 99588 osservazioni.\n",
      "Calcolata log-RV per frequenza '1D'. Trovate 1289 osservazioni.\n",
      "Calcolata log-RV per frequenza '1h'. Trovate 8984 osservazioni.\n",
      "Calcolata log-RV per frequenza '5min'. Trovate 99676 osservazioni.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simonepsn/.local/lib/python3.13/site-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: divide by zero encountered in log\n",
      "  result = func(self.values, **kwargs)\n",
      "/home/simonepsn/.local/lib/python3.13/site-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: divide by zero encountered in log\n",
      "  result = func(self.values, **kwargs)\n",
      "/home/simonepsn/.local/lib/python3.13/site-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: divide by zero encountered in log\n",
      "  result = func(self.values, **kwargs)\n",
      "/home/simonepsn/.local/lib/python3.13/site-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: divide by zero encountered in log\n",
      "  result = func(self.values, **kwargs)\n",
      "/home/simonepsn/.local/lib/python3.13/site-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: divide by zero encountered in log\n",
      "  result = func(self.values, **kwargs)\n",
      "/home/simonepsn/.local/lib/python3.13/site-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: divide by zero encountered in log\n",
      "  result = func(self.values, **kwargs)\n",
      "/home/simonepsn/.local/lib/python3.13/site-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: divide by zero encountered in log\n",
      "  result = func(self.values, **kwargs)\n",
      "/home/simonepsn/.local/lib/python3.13/site-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: divide by zero encountered in log\n",
      "  result = func(self.values, **kwargs)\n",
      "/home/simonepsn/.local/lib/python3.13/site-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: divide by zero encountered in log\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def calculate_log_rv(df, price_col='Close', resample_freq='1D'):\n",
    "    \"\"\"\n",
    "    Calcola la log realized volatility (log-RV) da dati ad alta frequenza.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con indice di tipo Datetime e colonna dei prezzi.\n",
    "        price_col (str): Nome della colonna dei prezzi.\n",
    "        resample_freq (str): Frequenza per il resampling (es. '1D', '4H', '1H').\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Serie della log realized volatility con timestamp.\n",
    "    \"\"\"\n",
    "\n",
    "    log_returns = np.log(df[price_col]).diff().dropna()\n",
    "    \n",
    "    # 2. Eleva al quadrato i log-return\n",
    "    squared_log_returns = log_returns**2\n",
    "    \n",
    "    # 3. Resample sommando i return al quadrato per ottenere la RV\n",
    "    # Il timestamp viene mantenuto automaticamente come indice\n",
    "    realized_variance = squared_log_returns.resample(resample_freq).sum()\n",
    "    \n",
    "    # 4. Calcola il logaritmo della RV, gestendo valori nulli o negativi\n",
    "    log_realized_variance = np.log(realized_variance)\n",
    "    \n",
    "    # Rimuovi i valori infiniti o mancanti (es. weekend senza trading)\n",
    "    log_realized_variance = log_realized_variance.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    \n",
    "    print(f\"Calcolata log-RV per frequenza '{resample_freq}'. Trovate {len(log_realized_variance)} osservazioni.\")\n",
    "    return log_realized_variance\n",
    "\n",
    "\n",
    "lrv_d_aapl = calculate_log_rv(df_aapl, price_col=['close'], resample_freq='1D')\n",
    "lrv_h_aapl = calculate_log_rv(df_aapl, price_col=['close'], resample_freq='1h')\n",
    "lrv_5m_aapl = calculate_log_rv(df_aapl, price_col=['close'], resample_freq='5min')\n",
    "\n",
    "lrv_d_nflx = calculate_log_rv(df_nflx, price_col=['close'], resample_freq='1D')\n",
    "lrv_h_nflx = calculate_log_rv(df_nflx, price_col=['close'], resample_freq='1h')\n",
    "lrv_5m_nflx = calculate_log_rv(df_nflx, price_col=['close'], resample_freq='5min')\n",
    "\n",
    "lrv_d_tsla = calculate_log_rv(df_tsla, price_col=['close'], resample_freq='1D')\n",
    "lrv_h_tsla = calculate_log_rv(df_tsla, price_col=['close'], resample_freq='1h')\n",
    "lrv_5m_tsla = calculate_log_rv(df_tsla, price_col=['close'], resample_freq='5min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de60137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFSV\n",
    "\n",
    "def theoretical_acf(h, n_lags):\n",
    "    \"\"\"Calcola l'ACF teorica per un processo fBm.\"\"\"\n",
    "    lags = np.arange(1, n_lags + 1)\n",
    "    # Formula dell'autocovarianza per un fBm incrementale\n",
    "    autocov = 0.5 * (np.abs(lags - 1)**(2*h) - 2 * np.abs(lags)**(2*h) + np.abs(lags + 1)**(2*h))\n",
    "    return autocov\n",
    "\n",
    "def est_parameters(log_rv_series, max_lags=100):\n",
    "    \"\"\"\n",
    "    Stima l'esponente di Hurst H e la vol-of-vol nu.\n",
    "\n",
    "    Args:\n",
    "        log_rv_series (pandas.Series): Serie della log realized variance.\n",
    "        max_lags (int): Numero di lag da usare per il matching dell'ACF.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (h_estimated, nu_estimated)\n",
    "    \"\"\"\n",
    "    # 1. Stima di H\n",
    "    empirical_acf_vals = acf(log_rv_series, nlags=max_lags, fft=True)[1:]\n",
    "    \n",
    "    def objective_function(h):\n",
    "\n",
    "        theoretical_acf_vals = theoretical_acf(h[0], max_lags)\n",
    "        # Minimizza la somma dei quadrati degli errori\n",
    "        return np.sum((empirical_acf_vals - theoretical_acf_vals)**2)\n",
    "\n",
    "    # Esegui la minimizzazione per trovare H\n",
    "    # Usiamo un valore iniziale ragionevole per H (es. 0.1)\n",
    "    result = minimize(objective_function, x0=[0.1], bounds=[(0.01, 10)])\n",
    "    h_estimated = result.x[0]\n",
    "\n",
    "    # 2. Stima di nu\n",
    "    # Var(log_sigma_t) = nu^2 * t^(2H)\n",
    "    # Per t=1 (passo giornaliero), Var(log_RV) ≈ nu^2\n",
    "    # Quindi, nu ≈ std(log_RV)\n",
    "    nu_estimated = np.std(log_rv_series)\n",
    "    \n",
    "    return h_estimated, nu_estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c31e5b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fbm_covariance_matrix(times, h):\n",
    "    \"\"\"\n",
    "    Costruisce la matrice di covarianza per un fBm ai tempi specificati.\n",
    "\n",
    "    Args:\n",
    "        times (np.array): Array di istanti temporali.\n",
    "        h (float): Esponente di Hurst.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Matrice di covarianza.\n",
    "    \"\"\"\n",
    "    n = len(times)\n",
    "    cov_matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            t_i = times[i]\n",
    "            t_j = times[j]\n",
    "            cov_matrix[i, j] = 0.5 * (\n",
    "                np.power(t_i, 2 * h) + \n",
    "                np.power(t_j, 2 * h) - \n",
    "                np.power(np.abs(t_i - t_j), 2 * h)\n",
    "            )\n",
    "    return cov_matrix\n",
    "\n",
    "def forecast_RFSV(past_log_rv_series, h, nu, horizon, n_sims=1):\n",
    "    \"\"\"\n",
    "    Esegue una previsione condizionale per il modello RFSV.\n",
    "\n",
    "    Args:\n",
    "        past_log_rv_series (pd.Series): Serie storica di log-RV.\n",
    "        h (float): Esponente di Hurst.\n",
    "        nu (float): Vol-of-vol.\n",
    "        horizon (int): Orizzonte di previsione.\n",
    "        n_sims (int): Numero di percorsi futuri da simulare.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Matrice di previsioni (n_sims, horizon). \n",
    "                  Se n_sims=1, restituisce la media condizionale.\n",
    "    \"\"\"\n",
    "    n_past = len(past_log_rv_series)\n",
    "    \n",
    "    # Definisci gli istanti temporali per il passato e il futuro\n",
    "    past_times = np.arange(1, n_past + 1)\n",
    "    future_times = np.arange(n_past + 1, n_past + horizon + 1)\n",
    "    all_times = np.concatenate([past_times, future_times])\n",
    "\n",
    "    # 1. Costruisci la matrice di covarianza globale (per W, non per log_sigma)\n",
    "    # la covarianza del processo log_sigma è nu^2 * Cov(W)\n",
    "    cov_global = build_fbm_covariance_matrix(all_times, h)\n",
    "    \n",
    "    # 2. Partiziona la matrice\n",
    "    sigma_pp = cov_global[:n_past, :n_past]\n",
    "    sigma_ff = cov_global[n_past:, n_past:]\n",
    "    sigma_fp = cov_global[n_past:, :n_past]\n",
    "    sigma_pf = sigma_fp.T\n",
    "\n",
    "    # 3. Calcola i componenti per la media e covarianza condizionali\n",
    "    # È la parte computazionalmente più costosa: inversione di sigma_pp\n",
    "    try:\n",
    "        sigma_pp_inv = np.linalg.inv(sigma_pp)\n",
    "    except np.linalg.LinAlgError:\n",
    "        # Usa la pseudo-inversa se la matrice è singolare\n",
    "        sigma_pp_inv = np.linalg.pinv(sigma_pp)\n",
    "\n",
    "    # Il processo che osserviamo è log_sigma = nu * W\n",
    "    # Quindi, il passato osservato di W è W_p = past_log_rv_series / nu\n",
    "    past_W = past_log_rv_series.values / nu\n",
    "    \n",
    "    # Media condizionale (per il processo W)\n",
    "    mean_cond_W = sigma_fp @ sigma_pp_inv @ past_W\n",
    "    \n",
    "    # Covarianza condizionale (per il processo W)\n",
    "    cov_cond_W = sigma_ff - sigma_fp @ sigma_pp_inv @ sigma_pf\n",
    "    \n",
    "    # 4. Simula dal processo condizionale\n",
    "    # Se n_sims > 1, campiona. Altrimenti, restituisci solo la media.\n",
    "    if n_sims > 1:\n",
    "        # Usa la decomposizione di Cholesky per simulare in modo efficiente\n",
    "        # Aggiungiamo una piccola quantità alla diagonale per stabilità numerica\n",
    "        L = cholesky(cov_cond_W + np.eye(horizon) * 1e-8, lower=True)\n",
    "        # Genera campioni casuali normali\n",
    "        z = np.random.normal(size=(horizon, n_sims))\n",
    "        # Simula i percorsi\n",
    "        simulated_paths_W = mean_cond_W[:, np.newaxis] + L @ z\n",
    "        # Riconverti in percorsi di log_sigma\n",
    "        simulated_paths_log_sigma = nu * simulated_paths_W.T\n",
    "        return simulated_paths_log_sigma\n",
    "    else:\n",
    "        # Se vuoi solo il \"best guess\", restituisci la media condizionale\n",
    "        mean_forecast_log_sigma = nu * mean_cond_W\n",
    "        return mean_forecast_log_sigma.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf8811",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_est_d_aapl, nu_est_d_aapl = est_parameters(lrv_d_aapl, max_lags=100)\n",
    "h_est_h_aapl, nu_est_h_aapl = est_parameters(lrv_h_aapl, max_lags=100)\n",
    "h_est_5m_aapl, nu_est_5m_aapl = est_parameters(lrv_5m_aapl, max_lags=100)\n",
    "\n",
    "print(\"1 day:\",h_est_d_aapl, nu_est_d_aapl)\n",
    "print(\"1 hour:\", h_est_h_aapl, nu_est_h_aapl)\n",
    "print(\"5 minutes\", h_est_5m_aapl, nu_est_5m_aapl)\n",
    "\n",
    "h_est_d_nflx, nu_est_d_nflx = est_parameters(lrv_d_nflx, max_lags=100)\n",
    "h_est_h_nflx, nu_est_h_nflx = est_parameters(lrv_h_nflx, max_lags=100)\n",
    "h_est_5m_nflx, nu_est_5m_nflx = est_parameters(lrv_5m_nflx, max_lags=100)\n",
    "\n",
    "print(\"1 day:\",h_est_d_nflx, nu_est_d_nflx)\n",
    "print(\"1 hour:\", h_est_h_nflx, nu_est_h_nflx)\n",
    "print(\"5 minutes\", h_est_5m_nflx, nu_est_5m_nflx)\n",
    "\n",
    "h_est_d_tsla, nu_est_d_tsla = est_parameters(lrv_d_tsla, max_lags=100)\n",
    "h_est_h_tsla, nu_est_h_tsla = est_parameters(lrv_h_tsla, max_lags=100)\n",
    "h_est_5m_tsla, nu_est_5m_tsla = est_parameters(lrv_5m_tsla, max_lags=100)\n",
    "\n",
    "print(\"1 day:\",h_est_d_tsla, nu_est_d_tsla)\n",
    "print(\"1 hour:\", h_est_h_tsla, nu_est_h_tsla)\n",
    "print(\"5 minutes\", h_est_5m_tsla, nu_est_5m_tsla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61d63188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_h_loglog(series, scales, q=1):\n",
    "    \"\"\"\n",
    "    Stima l'esponente di Hurst H usando una regressione log-log sui momenti.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): La serie temporale (preferibilmente detrendizzata).\n",
    "        scales (list of int): Lista di scale temporali (lag) da analizzare.\n",
    "        q (int): L'ordine del momento da usare (solitamente 1 o 2).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (H_stimato, R_squared)\n",
    "    \"\"\"\n",
    "    moments = []\n",
    "    for tau in scales:\n",
    "        # Calcola gli incrementi sulla scala 'tau' e rimuovi i NaN\n",
    "        increments = series.diff(tau).dropna()\n",
    "        # Calcola il momento di ordine q\n",
    "        moment = np.mean(np.abs(increments)**q)\n",
    "        moments.append(moment)\n",
    "    \n",
    "    # Prepara i dati per la regressione log-log\n",
    "    log_tau = np.log(scales)\n",
    "    log_moments = np.log(moments)\n",
    "    \n",
    "    # Aggiungi una costante per l'intercetta della regressione\n",
    "    X = sm.add_constant(log_tau)\n",
    "    y = log_moments\n",
    "    \n",
    "    # Esegui la regressione lineare (OLS)\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "    \n",
    "    # La pendenza è il coefficiente di log_tau\n",
    "    slope = results.params[1]\n",
    "    \n",
    "    # Ricava H dalla pendenza\n",
    "    h_estimated = slope / q\n",
    "    \n",
    "    return h_estimated, results.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "400b7513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stima di H (q=1): 0.1069\n",
      "R-squared della regressione (q=1): 0.9869\n",
      "\n",
      "Stima di H (q=2): 0.0946\n",
      "R-squared della regressione (q=2): 0.9826\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Usiamo una delle tue serie detrendizzate, ad esempio quella giornaliera di AAPL\n",
    "# Assumiamo che 'lvol_d_detrended_aapl' sia disponibile\n",
    "# lvol_d_detrended_aapl = detrend_series(calculate_log_rv(df_aapl, resample_freq='1D'))\n",
    "\n",
    "# Definiamo le scale temporali su cui analizzare il processo\n",
    "# È bene scegliere scale sia piccole che grandi\n",
    "time_scales = [1, 2, 5, 10, 20, 50, 100, 200]\n",
    "\n",
    "# Stima H usando il primo momento (q=1)\n",
    "H_loglog_q1, r2_q1 = estimate_h_loglog(lrv_d_aapl, time_scales, q=1)\n",
    "\n",
    "# Stima H usando il secondo momento (q=2)\n",
    "H_loglog_q2, r2_q2 = estimate_h_loglog(lrv_d_aapl, time_scales, q=2)\n",
    "\n",
    "\n",
    "print(f\"Stima di H (q=1): {H_loglog_q1:.4f}\")\n",
    "print(f\"R-squared della regressione (q=1): {r2_q1:.4f}\\n\")\n",
    "\n",
    "print(f\"Stima di H (q=2): {H_loglog_q2:.4f}\")\n",
    "print(f\"R-squared della regressione (q=2): {r2_q2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
